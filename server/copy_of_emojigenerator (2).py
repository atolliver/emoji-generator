# -*- coding: utf-8 -*-
"""Copy of emojigenerator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SJzYUFvtZRrKvMYnEhSUacqIz1u3kSyW
"""

import pandas as pd
import numpy as np
from PIL import Image
import io
import base64
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Embedding, Reshape, concatenate
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from google.colab import files
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2

import zipfile
IMAGE_SHAPE = (64, 64, 3)  #
DATA_PATH = "full_emoji.csv"
file_name = "full_emoji.csv.zip"
try:
    with zipfile.ZipFile(file_name, 'r') as zip_ref:
        zip_ref.extractall()
    print("File extracted successfully.")
except zipfile.BadZipFile:
    print("Error: Not a valid ZIP file or file is corrupted.")

try:
    data = pd.read_csv(DATA_PATH, dtype={'Apple': str})
    print("Data loaded successfully")
# except FileNotFoundError:
#     print(f"Error: Could not find dataset at {file_name}")
#     data = None
except Exception as e:
    print(f"Error loading data: {e}")
    data = None

 #model Training


def create_emoji_cnn(image_shape, num_emojis, embedding_dim=128, l2_reg=0.001):
    image_input = Input(shape=image_shape, name='image_input')

    conv1 = Conv2D(16, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(image_input)  # Reduced filters, L2 regularization
    pool1 = MaxPooling2D((2, 2))(conv1)
    dropout1 = Dropout(0.5)(pool1)

    conv2 = Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(dropout1)  # Reduced filters, L2 regularization
    pool2 = MaxPooling2D((2, 2))(conv2)
    dropout2 = Dropout(0.5)(pool2)

    flat = Flatten()(dropout2)

    # emoji input branch
    emoji_input = Input(shape=(1,), name='emoji_input')
    embedding = Embedding(input_dim=num_emojis, output_dim=embedding_dim)(emoji_input)
    emoji_flat = Flatten()(embedding)

    # to concatenate image and emoji features
    merged = concatenate([flat, emoji_flat])

    dense1 = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(merged)  # Reduced units, L2 regularization
    output_image = Dense(np.prod(image_shape), activation='sigmoid', kernel_regularizer=l2(l2_reg))(dense1)  # L2 regularization
    output_image = Reshape(image_shape)(output_image)  # Reshape to image dimensions

    model = Model(inputs=[image_input, emoji_input], outputs=[output_image])
    return model

data

# compiles the model

def train_model(model, image_data, emoji_data, epochs=10, batch_size=32):
     # for adjusting the learning rate
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='mse')  #the mean squared error loss
    # train the model
    history = model.fit([image_data, emoji_data], image_data, epochs=epochs, batch_size=batch_size, validation_split=0.2)

    # to graph the training history
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


# the  training
if __name__ == '__main__':
    if data is not None:
        image_paths = data["Apple"].tolist()
        emoji_labels = data["emoji"].tolist()
        label_encoder = LabelEncoder()
        emoji_data = label_encoder.fit_transform(emoji_labels)

        num_emojis = len(np.unique(emoji_data))
        image_data = []
        valid_indices = []
        for i, image_base64 in enumerate(image_paths):
            try:
                if not isinstance(image_base64, str):
                    print(f"Skipping non-string value at index {i}")
                    continue

                if ',' in image_base64:
                    image_data_decoded = base64.b64decode(image_base64.split(',')[1])
                else:
                    image_data_decoded = base64.b64decode(image_base64)
                image_io = io.BytesIO(image_data_decoded)
                img = Image.open(image_io).convert('RGB').resize(IMAGE_SHAPE[:2])
                img_array = np.array(img) / 255.0
                image_data.append(img_array)
                valid_indices.append(i)
            except Exception as e:
                print(f"Error loading image at index {i}: {e}")
                continue

        image_data = np.array(image_data)
        emoji_data = emoji_data[valid_indices]

        # splits the data into training and testing
        image_train, image_test, emoji_train, emoji_test = train_test_split(
            image_data, emoji_data, test_size=0.2, random_state=42
        )
        print(f"image_train.shape: {image_train.shape}")
        print(f"emoji_train.shape: {emoji_train.shape}")


        # creates and train the model
        model = create_emoji_cnn(IMAGE_SHAPE, num_emojis)
        train_model(model, image_train, emoji_train, epochs=10, batch_size=32)

test_loss = model.evaluate([image_test, emoji_test], image_test, batch_size=32)
print(f"Test Loss {test_loss:.4f}")

"""*italicized text*
Test Loss 0.0579
*italicized text*

"""

# makes the predictions on test data
predictions = model.predict([image_test, emoji_test])
predicted = np.argmax(predictions, axis =1)




# Visualize
n_samples = 5
plt.figure(figsize=(15, 6))
for i in range(n_samples):
    plt.subplot(2, n_samples, i+1)
    plt.imshow(image_test[i])
    plt.title(f"Original {i}")
    plt.axis('off')

    # predicted image
    plt.subplot(2, n_samples, i+n_samples+1)
    plt.imshow(predictions[i])
    plt.title(f"Predicted {i}")
    plt.axis('off')

plt.tight_layout()
plt.show()

print(predictions)



























